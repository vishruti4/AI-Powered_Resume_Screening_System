# ğŸ“… Internship Daily Report â€“ AI-Powered Resume Screening System

**ğŸ¯ Project Title:** Resume Screening System using NLP and Semantic Similarity  
**ğŸ§  Internship Domain:** Machine Learning / Natural Language Processing  
**âœï¸ Author:** Vishruti Parekh
**ğŸ“… Internship Period:** 12/05/2025 â€“ [End Date]  

---

## âœ… Overview

The objective of this internship project is to build an **AI-powered Resume Ranking System** that automatically matches resumes to job descriptions using NLP techniques such as **TF-IDF**, **Word2Vec**, and **BERT-based embeddings**.  
This document outlines the **day-wise progress report** for the entire model-building process.

---

## ğŸ“Œ Daily Log

### ğŸ“ **Day 1: Dataset Identification and Exploration**

- Explored multiple public datasets for resumes (e.g., Kaggleâ€™s Resume Dataset).
- Selected a labeled dataset containing fields like `Category` and `Resume`.
- Loaded the dataset using **Pandas** and conducted initial inspection.
- Verified dataset quality, checked for null values, and reviewed resume samples.

---

### ğŸ“ **Day 2: Research on Resume Ranking and Similarity Models**

- Conducted a literature review on automated resume screening systems.
- Compared techniques: **TF-IDF**, **Word2Vec (GloVe)**, **BERT**.
- Identified **cosine similarity** as the core matching metric.
- Decided to benchmark multiple embeddings for accuracy comparison.

---

### ğŸ“ **Day 3: Data Cleaning and Preprocessing**

- Created a `clean_text()` function using **spaCy** to:
  - Tokenize and lemmatize
  - Convert to lowercase
  - Remove punctuation and stopwords
- Applied cleaning to all resumes â†’ stored in `Cleaned_Resume`.
- Ensured normalized text for vectorization.

---

### ğŸ“ **Day 4: Text Extraction from PDF Resumes**

- Integrated `pdfminer.six` and `PyMuPDF (fitz)` for PDF parsing.
- Scripted batch PDF processing â†’ extracted structured content.
- Manually verified parsed text vs. original PDFs for accuracy.
- Ensured formatting consistency with the preprocessed dataset.

---

### ğŸ“ **Day 5: Updating and Structuring the Dataset**

- Merged extracted resumes with the original dataset.
- Created `final_resumes.csv` with:
  - `Resume`, `Cleaned_Resume`, `Category`, `Source`
- Removed duplicates, trimmed whitespaces, and handled encoding.

---

### ğŸ“ **Day 6: Implementing TF-IDF and BERT-based Embeddings**

- **TF-IDF:**
  - Applied `TfidfVectorizer` (1000 features).
  - Transformed job descriptions and computed cosine similarity.

- **BERT:**
  - Used `all-MiniLM-L6-v2` via `sentence-transformers`.
  - Encoded resumes and job descriptions into semantic vectors.
  - Achieved high-ranking accuracy.

---

### ğŸ“ **Day 7: Word2Vec Embeddings and Model Comparison**

- Loaded pre-trained **GloVe (100D)** via `gensim`.
- Created document-level vectors via averaged word embeddings.
- Compared cosine similarities across models.

#### ğŸ“Š **Model Comparison Table**

| **Model** | **Accuracy (Top-K Relevance)** | **Remarks**                       |
|----------|-------------------------------|----------------------------------|
| TF-IDF   | Medium (60â€“70%)               | Fast, lacks context              |
| Word2Vec | Medium-High (65â€“75%)          | Captures word semantics          |
| BERT     | High (80â€“90%)                 | Best contextual understanding    |

---

### ğŸ“ **Day 8: Finalizing and Saving the Model**

- Finalized **BERT** for deployment due to superior performance.
- Saved the following:
  - Clean dataset: `final_resumes.csv`
  - Models: `tfidf_model.pkl`, `resume_embeddings.pkl`
  - Scripts: ranking logic and similarity functions
- Documented full pipeline:

```txt
PDF/Text Resume â†’ Clean â†’ Vectorize â†’ Match with JD â†’ Rank
```
